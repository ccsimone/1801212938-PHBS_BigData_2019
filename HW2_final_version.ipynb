{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "df1 = pd.read_csv('climate_change_1.txt')\n",
    "df2 = pd.read_csv('climate_change_2.txt')\n",
    "\n",
    "# split the data into training data and test data\n",
    "data_split = 2006\n",
    "df1_train = df1[df1['Year'] <= data_split]\n",
    "df1_test = df1[df1['Year'] > data_split]\n",
    "df2_train = df2[df2['Year'] <= data_split]\n",
    "df2_test = df2[df2['Year'] > data_split]\n",
    "\n",
    "\n",
    "# distingush the data as label\n",
    "df1_train_temp = df1_train['Temp']\n",
    "df1_test_temp = df1_test['Temp']\n",
    "df2_train_temp = df2_train['Temp']\n",
    "df2_test_temp = df2_test['Temp']\n",
    "length_train1 = len(df1_train_temp)\n",
    "length_test1 = len(df1_test_temp)\n",
    "length_train2 = len(df2_train_temp)\n",
    "length_test2 = len(df2_test_temp)\n",
    "\n",
    "\n",
    "# distingush the data as feature\n",
    "df1_train_x = (df1_train.loc[:,'MEI':'Aerosols'])\n",
    "df1_train_x.insert(0, 'theta0', np.ones(length_train1))\n",
    "df1_test_x = df1_test.loc[:,'MEI':'Aerosols']\n",
    "df1_test_x.insert(0, 'theta0', np.ones(length_test1))\n",
    "\n",
    "df2_train_x = df2_train.loc[:,'MEI':'NO']\n",
    "df2_train_x.insert(0, 'theta0', np.ones(length_train2))\n",
    "df2_test_x = df2_test.loc[:,'MEI':'NO']\n",
    "df2_test_x.insert(0, 'theta0', np.ones(length_test2))\n",
    "\n",
    "\n",
    "#function closed_form_1()\n",
    "def closed_form_1(x,y):\n",
    "    xTx = mat(x).T @ mat(x)\n",
    "    xTy = mat(x).T @ mat(y).T\n",
    "    if linalg.det(xTx) == 0.0:\n",
    "        print(\"This matrix is singular, cannot do inverse\")\n",
    "        return\n",
    "    return xTx.I @ xTy\n",
    "\n",
    "\n",
    "theta1 = closed_form_1(df1_train_x, df1_train_temp)\n",
    "print(\"The estimate of Theta of climate_change_1 is \\n\", theta1, \"\\n-----------\")\n",
    "theta2 = closed_form_1(df2_train_x, df2_train_temp)\n",
    "print(\"The estimate of Theta of climate_change_2 is \\n\", theta2, \"\\n-----------\")\n",
    "\n",
    "\n",
    "# y = ax + b and in this case y = x @ theta \n",
    "#function to calculate R square\n",
    "def R_2(x, y, theta_):\n",
    "    n = len(y)\n",
    "    f = reshape(np.array(mat(x) @ theta_), n)\n",
    "    y = np.array(y)\n",
    "    y_ = np.ones(n) * sum(y) / n\n",
    "    ssr = sum((f - y_) ** 2)\n",
    "    sse = sum((y - f) ** 2)\n",
    "    sst = ssr + sse\n",
    "    return ssr/sst\n",
    "\n",
    "\n",
    "R_2_train1 = R_2(df1_train_x, df1_train_temp, theta1)\n",
    "R_2_test1 = R_2(df1_test_x, df1_test_temp, theta1)\n",
    "print(\"R square of train set 1 is\", R_2_train1)\n",
    "print(\"R square of test set 1 is\", R_2_test1)\n",
    "\n",
    "R_2_train2 = R_2(df2_train_x, df2_train_temp, theta2)\n",
    "R_2_test2 = R_2(df2_test_x, df2_test_temp, theta2)\n",
    "print(\"R square of train set 2 is\", R_2_train2)\n",
    "print(\"R square of test set 2 is\", R_2_test2)\n",
    "print(\"-----------\")\n",
    "\n",
    "# write a t-test to do the significance test\n",
    "def t_test(x,y,theta_,p):\n",
    "    n = len(y)\n",
    "    f = reshape(np.array(mat(x) @ theta_),n)\n",
    "    y = np.array(y)\n",
    "    xTx = mat(x).T @ mat(x)\n",
    "    #a, b = np.linalg.eig(xTx) # k = max(a) / min(a)\n",
    "    xTxI = xTx.I\n",
    "    xTxI = np.array(xTxI)\n",
    "    p1=p+1\n",
    "    c=np.zeros(p1)\n",
    "    for jj in range(p1):\n",
    "        c[jj] = xTxI[jj][jj]\n",
    "    sse = sum((y - f) ** 2)\n",
    "    sigma_h = sqrt(sse/(n-p1))\n",
    "    t = np.zeros(p1)\n",
    "    pv = np.zeros(p1)\n",
    "    pv1 = np.zeros(p1)\n",
    "    for i in range(p1):\n",
    "        t[i] = theta_[i] / sigma_h / sqrt(c[i])\n",
    "        pv[i] = 2 *(stats.t.sf(np.abs(t[i]),n-p1)) \n",
    "        #pv1[i]= 2*(1-stats.t.cdf(np.abs(t[i]),n-p1)) #just make a validation, it means the same with the last line\n",
    "        #The former mistake is from the absolute of t value\n",
    "    return t, pv, np.linalg.matrix_rank(xTx) #pv1\n",
    "\n",
    "test1 = t_test(df1_train_x, df1_train_temp,theta1,8)\n",
    "print(\"For climate_change_1: \\n\", \"t value is: \\n\", test1[0], \"\\n p value is: \\n\", test1[1], \"\\n rank is: \\n\",test1[2],\"\\n-----------\")\n",
    "test2 = t_test(df2_train_x, df2_train_temp,theta2,9)\n",
    "print(\"For climate_change_2: \\n\", \"t value is: \\n\", test2[0], \"\\n p value is: \\n\", test2[1], \"\\n rank is: \\n\",test2[2],\"\\n-----------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1 question 2:\n",
    "    \n",
    "   The function is： y = ax + b and in this case y = x @ theta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1 question 3:\n",
    "\n",
    "    1,2,5,6,7,8 are both significant, they are MEI,CO2,CFC-11,CFC-12,TSI,Aerosols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1 question 4:\n",
    "\n",
    "    For climate_change_2: The p value of the last parameter is larger than 0.05, so we cannot reject the null hypothesis. The consequene is that we cannot use the closed_form_1, and it results from the multicollinearity.\n",
    "        There are two necessary conditions and one less important condition:\n",
    "        1.XTX is inversible, i.e. the det of XTX is not equal to zero.\n",
    "        2.The rank of matrix XTX should equal to p+1. where p is the parameter in model. In that case, the climate_change_2 data has a rank 9 which is smaller that p+1=10, so there exists multicollinearity.\n",
    "        3.An important index to measure the severity of multicollinearity, the condition number k of xTx. k (XTX) is the ratio of the maximum eigenvalue to the minimum eigenvalue of the square matrix XTX. The condition number characterizes the difference in the eigenvalues of XTX. K <100 indicates that the degree of multicollinearity is small, 100 <k <1000 indicates that there is a medium layer or strong multicollinearity, and k> 1000 indicates that a serious Multicollinearity. But that condition is not necessary.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2 question 1：\n",
    "\n",
    "please see the graph in github for final format.\n",
    "linear model with L1 regularization： $$C(L1)=∑^n|yi−yi^ˆ|$$\n",
    "\n",
    "linear model with L2 regularization： $$C(L2)=∑^n(yi−yi^ˆ)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def closed_form_2(x, y, lamda):\n",
    "    I = np.identity(x.shape[1])\n",
    "    xTx1 = mat(x).T @ mat(x) + lamda * I\n",
    "    xTy = mat(x).T @ mat(y).T\n",
    "    return xTx1.I @ xTy\n",
    "\n",
    "theta1_r = closed_form_2(df1_train_x, df1_train_temp,3)\n",
    "print(\"The regulation estimate of Theta of climate_change_1 is \\n\", theta1_r, \"\\n-----------\")\n",
    "theta2_r = closed_form_2(df2_train_x, df2_train_temp,3)\n",
    "print(\"The regulation estimate of Theta of climate_change_2 is \\n\", theta2_r, \"\\n-----------\")\n",
    "\n",
    "R_2_train1_r = R_2(df1_train_x, df1_train_temp, theta1_r)\n",
    "R_2_test1_r = R_2(df1_test_x, df1_test_temp, theta1_r)\n",
    "print(\"R square of regulation train set 1 is\", R_2_train1_r)\n",
    "print(\"R square of regulation test set 1 is\", R_2_test1_r)\n",
    "\n",
    "R_2_train2_r = R_2(df2_train_x, df2_train_temp, theta2_r)\n",
    "R_2_test2_r = R_2(df2_test_x, df2_test_temp, theta2_r)\n",
    "print(\"R square of regulation train set 2 is\", R_2_train2_r)\n",
    "print(\"R square of regulation test set 2 is\", R_2_test2_r)\n",
    "print(\"-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2 question 3：\n",
    "\n",
    "R square of train set 1 is 0.750893277187922\n",
    "\n",
    "R square of test set 1 is 0.21622556651959746\n",
    "\n",
    "R square of regulation train set 1 is 0.6815401104630927\n",
    "\n",
    "R square of regulation test set 1 is 0.3519412583261267\n",
    "    \n",
    "   We can discover that the R square on test data of regulation set is higher than original data. It means that the regulation one is more robust.\n",
    "   The reason is that the L2 loss is calculated from the residuals. Because the residual between the fitted line and the noise is relatively large, and the L2 loss is considered on the basis of this squared, so the L2 loss will be larger.\n",
    "    This effect is that the fitting straight line we use will \"value\" the noise points more, and the whole fitting straight line will be biased towards the noise point, so it is more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda_list=np.array([20, 15, 10, 1, 0.1, 0.01, 0.001])\n",
    "R2_1_train = np.zeros(7)\n",
    "R2_1_test = np.zeros(7)\n",
    "R2_2_train = np.zeros(7)\n",
    "R2_2_test = np.zeros(7)\n",
    "    \n",
    "for i in range(7):\n",
    "    lam = lamda_list[i]\n",
    "\n",
    "    #print(\"************************************************\")\n",
    "    #print(\"when lambda is:\",i)\n",
    "    theta1_r = closed_form_2(df1_train_x, df1_train_temp,lam)\n",
    "    #print(\"The regulation estimate of Theta of climate_change_1 is \\n\", theta1_r, \"\\n-----------\")\n",
    "    theta2_r = closed_form_2(df2_train_x, df2_train_temp,lam)\n",
    "    #print(\"The regulation estimate of Theta of climate_change_2 is \\n\", theta2_r, \"\\n-----------\")\n",
    "\n",
    "    R2_1_train[i] = R_2(df1_train_x, df1_train_temp, theta1_r)\n",
    "    R2_1_test[i] = R_2(df1_test_x, df1_test_temp, theta1_r)\n",
    "    #print(\"R square of regulation train set 1 is\", R2_1_train[i])\n",
    "    #print(\"R square of regulation test set 1 is\", R2_1_test[i])\n",
    "\n",
    "    R2_2_train[i] = R_2(df2_train_x, df2_train_temp, theta2_r)\n",
    "    R2_2_test[i] = R_2(df2_test_x, df2_test_temp, theta2_r)\n",
    "    #print(\"R square of regulation train set 2 is\", R2_2_train[i])\n",
    "    #print(\"R square of regulation test set 2 is\", R2_2_test[i])\n",
    "    #print(\"-----------\")\n",
    "\n",
    "print(\"R square of regulation train set 1 is\", R2_1_train)\n",
    "print(\"R square of regulation test set 1 is\", R2_1_test)\n",
    "print(\"R square of regulation train set 2 is\", R2_2_train)\n",
    "print(\"R square of regulation test set 2 is\", R2_2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lamda_list=np.array([20, 15, 10, 1, 0.1, 0.01, 0.001])\n",
    "x_values = list(range(1,8))\n",
    "y1_values = R2_1_train\n",
    "y2_values = R2_1_test\n",
    "y3_values = R2_2_train\n",
    "y4_values = R2_2_test\n",
    "plt.scatter(x_values, y1_values, edgecolor='none', s=100, marker='|')\n",
    "plt.scatter(x_values, y2_values, edgecolor='none', s=100, marker='|')\n",
    "plt.scatter(x_values, y3_values, edgecolor='none', s=100, marker='*')\n",
    "plt.scatter(x_values, y4_values, edgecolor='none', s=100, marker='*')\n",
    "\n",
    "plt.title(\"R square\", fontsize=24)\n",
    "plt.xlabel(\"Value\", fontsize=14)\n",
    "plt.ylabel(\"Square of value\", fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cross validation to select the best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "df1 = pd.read_csv('climate_change_1.txt')\n",
    "df2 = pd.read_csv('climate_change_2.txt')\n",
    "\n",
    "# distingush the data as label\n",
    "df1_temp = df1['Temp']\n",
    "df2_temp = df2['Temp']\n",
    "length_1 = len(df1_temp)\n",
    "length_2 = len(df2_temp)\n",
    "\n",
    "# distingush the data as feature\n",
    "df1_x = (df1.loc[:,'MEI':'Aerosols'])\n",
    "df1_x.insert(0, 'theta0', np.ones(length_1))\n",
    "\n",
    "df2_x = df2.loc[:,'MEI':'NO']\n",
    "df2_x.insert(0, 'theta0', np.ones(length_2))\n",
    "\n",
    "#x,y = np.array(mat(df1_x)), np.array(mat(df1_temp))\n",
    "x,y = df1_x, df1_temp\n",
    "x1,y1 = df2_x, df2_temp\n",
    "\n",
    "lamda_list=np.array([20, 15, 10, 1, 0.1,0.01, 0.001,0.0001])\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y ,train_size=0.8)\n",
    "model = RidgeCV(alphas=lamda_list)\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "print(\"The best coef is \",model.coef_)\n",
    "print(\"The best intercept is \",model.intercept_)\n",
    "print(\"The best lambda is \",model.alpha_)\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1 = train_test_split(x1,y1 ,train_size=0.8)\n",
    "model1 = RidgeCV(alphas=lamda_list)\n",
    "model1.fit(X_train1, y_train1)\n",
    "y_predicted1 = model1.predict(X_test1)\n",
    "print(\"The best coef is \",model1.coef_)\n",
    "print(\"The best intercept is \",model1.intercept_)\n",
    "print(\"The best lambda is \",model1.alpha_)\n",
    "\n",
    "def R_2(y, y1):\n",
    "    n = len(y)\n",
    "    y = np.array(y)\n",
    "    y_ = np.ones(n) * sum(y) / n\n",
    "    ssr = sum((y1 - y_) ** 2)\n",
    "    sse = sum((y - y1) ** 2)\n",
    "    sst = ssr + sse\n",
    "    return ssr/sst\n",
    "\n",
    "R1 = R_2(y[246:308],y_predicted)\n",
    "print(\"R square of whole set 1 is\", R1)\n",
    "R2 = R_2(y1[246:308],y_predicted1)\n",
    "print(\"R square of whole set 2 is\", R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2 question 4:\n",
    "\n",
    "Each lambda corresponds to a set of weights. As the lambda increases, the weight gradually decreases. The best parameters are in one of these groups. To quantitatively find the best parameters, cross-validation is also required. \n",
    "\n",
    "The best lambda is 0.01.\n",
    "\n",
    "Print the R square graph, the smaller lambda has the better performance.\n",
    "\n",
    "The importance of cross validation: We do not have sufficient to do the tests. So in a given modeling sample, take out most of the samples to build a model, leave a small part of the sample to forecast using the model just established, and find the forecast error of this small sample, and record their sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3 question 1:\n",
    "Steps in stepwise regression analysis:\n",
    "1. For all factors, the regression equation is introduced one by one according to the degree of its influence on y (partial regression squared)\n",
    "2. Test all the variables contained in the regression equation at any time to see if they are still significant;\n",
    "3. Among the remaining unselected factors, select the one that has the greatest effect on y and test its significance;\n",
    "4. Significant, the equation is introduced, and not significant, the equation is not introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "df1 = pd.read_csv('climate_change_1.txt')\n",
    "df2 = pd.read_csv('climate_change_2.txt')\n",
    "\n",
    "# split the data into training data and test data\n",
    "data_split = 2006\n",
    "df1_train = df1[df1['Year'] <= data_split]\n",
    "df1_test = df1[df1['Year'] > data_split]\n",
    "df2_train = df2[df2['Year'] <= data_split]\n",
    "df2_test = df2[df2['Year'] > data_split]\n",
    "\n",
    "\n",
    "# distingush the data as label\n",
    "df1_train_temp = df1_train['Temp']\n",
    "df1_test_temp = df1_test['Temp']\n",
    "df2_train_temp = df2_train['Temp']\n",
    "df2_test_temp = df2_test['Temp']\n",
    "length_train1 = len(df1_train_temp)\n",
    "length_test1 = len(df1_test_temp)\n",
    "length_train2 = len(df2_train_temp)\n",
    "length_test2 = len(df2_test_temp)\n",
    "\n",
    "\n",
    "# distingush the data as feature\n",
    "df1_train_x = df1_train.loc[:,'MEI':'Aerosols']\n",
    "df1_train_x.insert(0, 'theta0', np.ones(length_train1))\n",
    "df1_test_x = df1_test.loc[:,'MEI':'Aerosols']\n",
    "df1_test_x.insert(0, 'theta0', np.ones(length_test1))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "X_train, X_test, Y_train, Y_test = df1_train_x, df1_test_x, df1_train_temp, df1_test_temp\n",
    "pca = PCA(n_components=5)\n",
    "# 对模型进行训练\n",
    "pca.fit(X_train)\n",
    "# 返回降维后据\n",
    "X_train = pca.transform(X_train)\n",
    "\n",
    "# 使用返回后的数据用线性回归模型进行建模\n",
    "\n",
    "import statsmodels.api as sm\n",
    "ols = sm.OLS(Y_train, X_train).fit()\n",
    "ols.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3 question 2:\n",
    "\n",
    "    When we use the PCA to contain only 5 variables, The answer is better. Because the most parameters are significant.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4:\n",
    "    $$θj=θj-α\\frac{\\partial J(θ0, θ1)}{\\partial θj}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "\n",
    "def gradientDescent(fn, partial_derivatives, n_variables, lr=0.1,\n",
    "                    max_iter=10000, tolerance=1e-5):\n",
    "    theta = [random() for _ in range(n_variables)]\n",
    "    y_cur = fn(*theta)\n",
    "    for i in range(max_iter):\n",
    "        # Calculate gradient of current theta.\n",
    "        gradient = [f(*theta) for f in partial_derivatives]\n",
    "        # Update the theta by the gradient.\n",
    "        for j in range(n_variables):\n",
    "            theta[j] -= gradient[j] * lr\n",
    "        # Check if converged or not.\n",
    "        y_cur, y_pre = fn(*theta), y_cur\n",
    "        if abs(y_pre - y_cur) < tolerance:\n",
    "            break\n",
    "    return theta, y_cur\n",
    "\n",
    "def f(x, y):\n",
    "    return (x + y - 3) ** 2 + (x + 2 * y - 5) ** 2 + 2\n",
    "\n",
    "\n",
    "def df_dx(x, y):\n",
    "    return 2 * (x + y - 3) + 2 * (x + 2 * y - 5)\n",
    "\n",
    "\n",
    "def df_dy(x, y):\n",
    "    return 2 * (x + y - 3) + 4 * (x + 2 * y - 5)\n",
    "\n",
    "def main():\n",
    "    print(\"Solve the minimum value of quadratic function:\")\n",
    "    n_variables = 2\n",
    "    theta, f_theta = gradientDescent(f, [df_dx, df_dy], n_variables)\n",
    "    theta = [round(x, 3) for x in theta]\n",
    "    print(\"The solution is: theta %s, f(theta) %.2f.\\n\" % (theta, f_theta))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(inX):\n",
    "    return 1.0/(1+exp(-inX))\n",
    "\n",
    "def gradDescent(dataMatIn, classLabels):\n",
    "    dataMatrix = mat(dataMatIn)             #convert to NumPy matrix\n",
    "    labelMat = mat(classLabels).transpose() #convert to NumPy matrix\n",
    "    m,n = shape(dataMatrix)\n",
    "    alpha = 0.001\n",
    "    maxCycles = 500\n",
    "    weights = ones((n,1))\n",
    "    for k in range(maxCycles):              #heavy on matrix operations\n",
    "        h = sigmoid(dataMatrix*weights)     #matrix mult\n",
    "        error = (labelMat - h)              #vector subtraction\n",
    "        weights = weights - alpha * dataMatrix.transpose()* error #matrix mult\n",
    "    return weights\n",
    "\n",
    "g1=gradDescent(x,y)\n",
    "g2=gradDescent(x1,y1)\n",
    "print(g1)\n",
    "print(g2)\n",
    "\n",
    "def R_2(x, y, theta_):\n",
    "    n = len(y)\n",
    "    f = reshape(np.array(mat(x) @ theta_), n)\n",
    "    y = np.array(y)\n",
    "    y_ = np.ones(n) * sum(y) / n\n",
    "    ssr = sum((f - y_) ** 2)\n",
    "    sse = sum((y - f) ** 2)\n",
    "    sst = ssr + sse\n",
    "    return ssr/sst\n",
    "\n",
    "\n",
    "R_2_train1 = R_2(df1_train_x, df1_train_temp, g1)\n",
    "R_2_test1 = R_2(df1_test_x, df1_test_temp, g1)\n",
    "print(\"R square of train set 1 is\", R_2_train1)\n",
    "print(\"R square of test set 1 is\", R_2_test1)\n",
    "\n",
    "R_2_train2 = R_2(df2_train_x, df2_train_temp, g2)\n",
    "R_2_test2 = R_2(df2_test_x, df2_test_temp, g2)\n",
    "print(\"R square of train set 2 is\", R_2_train2)\n",
    "print(\"R square of test set 2 is\", R_2_test2)\n",
    "print(\"-----------\")\n",
    "\n",
    "# write a t-test to do the significance test\n",
    "def t_test(x,y,theta_,p):\n",
    "    n = len(y)\n",
    "    f = reshape(np.array(mat(x) @ theta_),n)\n",
    "    y = np.array(y)\n",
    "    xTx = mat(x).T @ mat(x)\n",
    "    #a, b = np.linalg.eig(xTx) # k = max(a) / min(a)\n",
    "    xTxI = xTx.I\n",
    "    xTxI = np.array(xTxI)\n",
    "    p1=p+1\n",
    "    c=np.zeros(p1)\n",
    "    for jj in range(p1):\n",
    "        c[jj] = xTxI[jj][jj]\n",
    "    sse = sum((y - f) ** 2)\n",
    "    sigma_h = sqrt(sse/(n-p1))\n",
    "    t = np.zeros(p1)\n",
    "    pv = np.zeros(p1)\n",
    "    pv1 = np.zeros(p1)\n",
    "    for i in range(p1):\n",
    "        t[i] = theta_[i] / sigma_h / sqrt(c[i])\n",
    "        pv[i] = 2 *(stats.t.sf(np.abs(t[i]),n-p1)) \n",
    "        #pv1[i]= 2*(1-stats.t.cdf(np.abs(t[i]),n-p1)) #just make a validation, it means the same with the last line\n",
    "        #The former mistake is from the absolute of t value\n",
    "    return t, pv, np.linalg.matrix_rank(xTx) #pv1\n",
    "\n",
    "test1 = t_test(df1_train_x, df1_train_temp,g1,8)\n",
    "print(\"For climate_change_1: \\n\", \"t value is: \\n\", test1[0], \"\\n p value is: \\n\", test1[1], \"\\n rank is: \\n\",test1[2],\"\\n-----------\")\n",
    "test2 = t_test(df2_train_x, df2_train_temp,g2,9)\n",
    "print(\"For climate_change_2: \\n\", \"t value is: \\n\", test2[0], \"\\n p value is: \\n\", test2[1], \"\\n rank is: \\n\",test2[2],\"\\n-----------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is better in gradient descent case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
